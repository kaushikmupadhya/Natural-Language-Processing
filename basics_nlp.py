# -*- coding: utf-8 -*-
"""Basics_NLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AEnGCBRsmDwBl_02BdnnYnUZN6zxlOb_
"""

from google.colab import drive
drive.mount('/content/drive')

"""# NLP Basics: What is Natural Language Processing & the Natural Language Toolkit?

### How to install NLTK on your local machine

Both sets of instructions below assume you already have Python installed. These instructions are taken directly from [http://www.nltk.org/install.html](http://www.nltk.org/install.html).

**Mac/Unix**

From the terminal:
1. Install NLTK: run `pip install -U nltk`
2. Test installation: run `python` then type `import nltk`

**Windows**

1. Install NLTK: [http://pypi.python.org/pypi/nltk](http://pypi.python.org/pypi/nltk)
2. Test installation: `Start>Python35`, then type `import nltk`

### Download NLTK data
"""

import nltk
nltk.download('popular') #Downloading popular packages

dir(nltk)

"""### What can you do with NLTK?"""

from nltk.corpus import stopwords

stopwords.words('english')[0:500:25]

"""## Reading the data

### Read in semi-structured text data
"""

# Read in the raw text
rawData = open("/content/drive/MyDrive/Colab Notebooks/SMSSpamCollection.tsv").read()

# Print the raw data
rawData[0:500]

parsedData = rawData.replace('\t', '\n').split('\n')

parsedData[0:5]

labelList = parsedData[0::2]
textList = parsedData[1::2]

print(labelList[0:5])
print(textList[0:5])

import pandas as pd

fullCorpus = pd.DataFrame({
    'label': labelList,
    'body_list': textList
})

fullCorpus.head()

print(len(labelList))
print(len(textList))

print(labelList[-5:])

fullCorpus = pd.DataFrame({
    'label': labelList[:-1],
    'body_list': textList
})

fullCorpus.head()

dataset = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/SMSSpamCollection.tsv", sep="\t", header=None)
dataset.head()

"""##Exploring the dataset

### Read in text data
"""

import pandas as pd

fullCorpus = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/SMSSpamCollection.tsv', sep='\t', header=None)
fullCorpus.columns = ['label', 'body_text']

fullCorpus.head()

"""### Explore the dataset"""

# What is the shape of the dataset?

print("Input data has {} rows and {} columns".format(len(fullCorpus), len(fullCorpus.columns)))

# How many spam/ham are there?

print("Out of {} rows, {} are spam, {} are ham".format(len(fullCorpus),
                                                       len(fullCorpus[fullCorpus['label']=='spam']),
                                                       len(fullCorpus[fullCorpus['label']=='ham'])))

# How much missing data is there?

print("Number of null in label: {}".format(fullCorpus['label'].isnull().sum()))
print("Number of null in text: {}".format(fullCorpus['body_text'].isnull().sum()))

"""## Learning how to use regular expressions

### Using regular expressions in Python

Python's `re` package is the most commonly used regex resource. More details can be found [here](https://docs.python.org/3/library/re.html).
"""

import re

re_test = 'This is a made up string to test 2 different regex methods'
re_test_messy = 'This      is a made up     string to test 2    different regex methods'
re_test_messy1 = 'This-is-a-made/up.string*to>>>>test----2""""""different~regex-methods'

"""### Splitting a sentence into a list of words"""

re.split('\s', re_test)

re.split('\s', re_test_messy)

re.split('\s+', re_test_messy)

re.split('\s+', re_test_messy1)

re.findall('\S+', re_test)

re.findall('\S+', re_test_messy)

re.findall('\S+', re_test_messy1)

re.findall('\w+', re_test_messy1)

"""### Replacing a specific string"""

pep8_test = 'I try to follow PEP8 guidelines'
pep7_test = 'I try to follow PEP7 guidelines'
peep8_test = 'I try to follow PEEP8 guidelines'

import re

re.findall('[a-z]+', pep8_test)

re.findall('[A-Z]+', pep8_test)

re.sub('[A-Z]+[0-9]+', 'PEP8 Python Styleguide', peep8_test)

"""### Other examples of regex methods

- re.search()
- re.match()
- re.fullmatch()
- re.finditer()
- re.escape()

## Implementing a pipeline to clean text

### Pre-processing text data

Cleaning up the text data is necessary to highlight attributes that you're going to want your machine learning system to pick up on. Cleaning (or pre-processing) the data typically consists of a number of steps:
1. **Remove punctuation**
2. **Tokenization**
3. **Remove stopwords**
4. Lemmatize/Stem

The first three steps are covered in this chapter as they're implemented in pretty much any text cleaning pipeline. Lemmatizing and stemming are covered in the next chapter as they're helpful but not critical.
"""

import pandas as pd
pd.set_option('display.max_colwidth', 100)

data = pd.read_csv("/content/drive/MyDrive/Colab Notebooks/SMSSpamCollection.tsv", sep='\t', header=None)
data.columns = ['label', 'body_text']

data.head()

# What does the cleaned version look like?
data_cleaned = pd.read_csv("SMSSpamCollection_cleaned.tsv", sep='\t')
data_cleaned.head()

"""### Remove punctuation"""

import string
string.punctuation

"I like NLP." == "I like NLP"

def remove_punct(text):
    text_nopunct = "".join([char for char in text if char not in string.punctuation])
    return text_nopunct

data['body_text_clean'] = data['body_text'].apply(lambda x: remove_punct(x))

data.head()

"""### Tokenization"""

import re

def tokenize(text):
    tokens = re.split('\W+', text)
    return tokens

data['body_text_tokenized'] = data['body_text_clean'].apply(lambda x: tokenize(x.lower()))

data.head()

'NLP' == 'nlp'

"""### Remove stopwords"""

import nltk

stopword = nltk.corpus.stopwords.words('english')
print(stopword)

def remove_stopwords(tokenized_list):
    text = [word for word in tokenized_list if word not in stopword]
    return text

data['body_text_nostop'] = data['body_text_tokenized'].apply(lambda x: remove_stopwords(x))

data.head()

